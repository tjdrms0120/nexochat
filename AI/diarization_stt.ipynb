{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSXRvKegizEd"
      },
      "source": [
        "### Prepare Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fMONkam7a3pE"
      },
      "outputs": [],
      "source": [
        "# speechbrain (used for speaker embedding)\n",
        "!pip install torch==2.3.1 torchaudio==2.3.1 torch-audiomentations==0.11.1\n",
        "!pip install -qq speechbrain==1.0.0\n",
        "\n",
        "# pyannote.audio (used for speaker diarization)\n",
        "!pip install -qq pyannote.audio==3.3.1\n",
        "\n",
        "# OpenAI whisper (used for automatic speech recognition)\n",
        "!pip install -qq git+https://github.com/openai/whisper.git\n",
        "!pip install -qq ipython==8.12.2\n",
        "\n",
        "# deVAD\n",
        "!pip install -U denoiser\n",
        "!pip install soundfile\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6-4xVV6wa5_x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "import io\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "from IPython.display import Audio\n",
        "\n",
        "from subprocess import run, CalledProcessError, PIPE\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# pyannote\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "# STT\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjHCANFCi8kR"
      },
      "source": [
        "## 1. STT, 화자분리 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7J6i3o-xa7gW"
      },
      "outputs": [],
      "source": [
        "# Hugging Face 토큰\n",
        "HUGGINGFACE_TOKEN = \"token\"\n",
        "\n",
        "# STT 모델, 프로세서, 토크나이저를 각각의 저장소에서 로드\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"NexoChatFuture/whisper-small-youtube-extra\", token=HUGGINGFACE_TOKEN)\n",
        "processor = AutoProcessor.from_pretrained(\"NexoChatFuture/whisper-small-youtube-extra\", token=HUGGINGFACE_TOKEN)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NexoChatFuture/whisper-small-youtube-extra\", token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "# 화자분리 모델 로드\n",
        "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "u_KTEALXa9jz"
      },
      "outputs": [],
      "source": [
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "pipeline.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIEpvI1rjM27"
      },
      "source": [
        "## 2. 화자분리 및 STT 전사 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO2nX0LSj-nx"
      },
      "source": [
        "### 2-1. 오디오 로드 함수 및 Constants 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oTonMHJjSD1"
      },
      "source": [
        "- whisper input을 위해 sampling rate를 16000로 설정하고, 원 음성을 30분 단위로 나누어 전사한다.\n",
        "\n",
        "     - SR = 16000\n",
        "     - CHUNK_DURATION = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rd2KPKkPjl1m"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "SR = 16000\n",
        "CHUNK_DURATION = 30  # 30초 단위"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQGcoxayjm5V"
      },
      "source": [
        "- whisper 모델을 오디오 데이터를 input으로 받고, 화자분리 모델은 오디오의 경로를 필요로 한다.\n",
        "- 따라서, 오디오 전처리 후 변환된 오디오와 경로를 모두 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hVKNS7BjjMZl"
      },
      "outputs": [],
      "source": [
        "# 오디오 로드\n",
        "def load_audio(path):\n",
        "    output_path = './output_audio.wav'\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-nostdin\",\n",
        "        \"-threads\", \"0\",\n",
        "        \"-i\", path,\n",
        "        \"-f\", \"f32le\",\n",
        "        \"-ac\", \"1\",\n",
        "        \"-acodec\", \"pcm_f32le\",\n",
        "        \"-ar\", str(SR),\n",
        "        \"-\"\n",
        "    ]\n",
        "    out = subprocess.run(cmd, capture_output=True, check=True).stdout\n",
        "    audio = np.frombuffer(out, np.float32)\n",
        "    sf.write(output_path, audio, SR)\n",
        "    return audio, output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxuzfg1YkD2I"
      },
      "source": [
        "### 2-2. STT 전사 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f4G4fezokL-t"
      },
      "outputs": [],
      "source": [
        "# 30초 단위로 STT 전사 진행\n",
        "def return_transcription(audio_data):\n",
        "    audio = np.frombuffer(audio_data.read(), dtype=np.float32)\n",
        "    total_duration = len(audio) / SR\n",
        "    transcriptions = []\n",
        "\n",
        "    for start in range(0, int(total_duration), CHUNK_DURATION):\n",
        "        end = min(start + CHUNK_DURATION, total_duration)\n",
        "        chunk = audio[int(start * SR):int(end * SR)]\n",
        "        inputs = processor(chunk, return_tensors=\"pt\", sampling_rate=SR)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = model.generate(inputs.input_features)\n",
        "\n",
        "        transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "        transcriptions.append(transcription)\n",
        "\n",
        "    # 30초 단위로 전사 진행 후 결과 합침\n",
        "    return ' '.join(transcriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNMRy5gHkgPU"
      },
      "source": [
        "### 2-3. 화자분리 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UojEu0Rkrho"
      },
      "source": [
        "`merge_speaker_rows`\n",
        "- 단일 화자의 연속 발화 중 무음 구간이 감지되면, 화자분리 모델은 해당 구간을 기준으로 음성을 분할한다.\n",
        "- 결과적으로 동일 화자의 발화도 복수의 세그먼트로 분할될 수 있다.\n",
        "- STT 모델은 짧은 음성에서 할루시네이션이 자주 발생한다.\n",
        "- 따라서, STT 모델의 전사 성능과 가독성 향상을 위해 같은 화자의 연속된 발화 세그먼트를 병합한다.\n",
        "\n",
        "`rename_speakers`\n",
        "- 프로젝트의 목표인 회의 데이터 처리를 위해, 화자분리 모델에서 출력하는 화자 변수인 'speaker' 를 '참석자' 로 변경한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X8n9n60ukq5A"
      },
      "outputs": [],
      "source": [
        "# 같은 화자 병합\n",
        "def merge_speaker_rows(df):\n",
        "    merged_data = []\n",
        "    current_speaker = None\n",
        "    current_start = None\n",
        "    current_end = None\n",
        "    current_audio = []\n",
        "    current_transcription = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if row['speakers'] == current_speaker:\n",
        "            current_end = row['end_timestamp']\n",
        "            current_audio.append(row['audio'])\n",
        "            current_transcription.append(row.get('transcription', ''))\n",
        "        else:\n",
        "            if current_speaker is not None:\n",
        "                merged_data.append({\n",
        "                    'speakers': current_speaker,\n",
        "                    'start_timestamp': current_start,\n",
        "                    'end_timestamp': current_end,\n",
        "                    'audio': b''.join(current_audio),\n",
        "                    'transcription': ' '.join(current_transcription).strip()\n",
        "                })\n",
        "            current_speaker = row['speakers']\n",
        "            current_start = row['start_timestamp']\n",
        "            current_end = row['end_timestamp']\n",
        "            current_audio = [row['audio']]\n",
        "            current_transcription = [row.get('transcription', '')]\n",
        "\n",
        "    if current_speaker is not None:\n",
        "        merged_data.append({\n",
        "            'speakers': current_speaker,\n",
        "            'start_timestamp': current_start,\n",
        "            'end_timestamp': current_end,\n",
        "            'audio': b''.join(current_audio),\n",
        "            'transcription': ' '.join(current_transcription).strip()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(merged_data)\n",
        "\n",
        "# 'speaker' -> '참석자 n' 으로 변경\n",
        "def rename_speakers(df):\n",
        "    unique_speakers = df['speakers'].unique()\n",
        "    speaker_map = {speaker: f'참석자 {i+1}' for i, speaker in enumerate(unique_speakers)}\n",
        "    df['speakers'] = df['speakers'].map(speaker_map)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grA_AnktmvfD"
      },
      "source": [
        "### 2-4. 화자분리 및 STT 전사 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-d2GEUDmyVE"
      },
      "source": [
        "- 화자분리를 먼저 진행한다.\n",
        "- 분리된 단일 화자의 연속된 세그먼트를 병합하여 STT 모델에 입력시킨다.\n",
        "- 결측치 제거 후 최종 결과를 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9wZcujACbCQB"
      },
      "outputs": [],
      "source": [
        "# 화자분리 및 STT 전사\n",
        "def speaker_diarize(path, num_speakers):\n",
        "    start_time = time.time()\n",
        "    audio, audio_path = load_audio(path)\n",
        "    speakers = []\n",
        "    start_timestamp = []\n",
        "    end_timestamp = []\n",
        "    audio_segments = []\n",
        "\n",
        "    # 화자분리 진행\n",
        "    diarization = pipeline({'uri': 'unique_audio_identifier', 'audio': audio_path}, num_speakers=num_speakers)\n",
        "    print(f'Finished with diarization, took {time.time() - start_time} sec')\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 화자분리 완료된 timestamp 에 맞춰서 STT 전사 진행\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-nostdin\",\n",
        "            \"-i\", path,\n",
        "            \"-ss\", str(turn.start),\n",
        "            \"-to\", str(turn.end),\n",
        "            \"-f\", \"f32le\",\n",
        "            \"-ac\", \"1\",\n",
        "            \"-acodec\", \"pcm_f32le\",\n",
        "            \"-ar\", str(SR),\n",
        "            \"pipe:1\"\n",
        "        ]\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        stdout, stderr = process.communicate()\n",
        "\n",
        "        if process.returncode != 0:\n",
        "            raise Exception(f\"Error occurred during ffmpeg process: {stderr.decode()}\")\n",
        "\n",
        "        audio_data = io.BytesIO(stdout)\n",
        "\n",
        "        speakers.append(speaker)\n",
        "        start_timestamp.append(turn.start)\n",
        "        end_timestamp.append(turn.end)\n",
        "        audio_segments.append(audio_data.read())\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'speakers': speakers,\n",
        "        'start_timestamp': start_timestamp,\n",
        "        'end_timestamp': end_timestamp,\n",
        "        'audio': audio_segments\n",
        "    })\n",
        "\n",
        "    # 같은 화자의 연속된 발화는 한 음성으로 합친 후 전사\n",
        "\n",
        "    merged_df = merge_speaker_rows(df)\n",
        "\n",
        "    # STT 전사 진행\n",
        "    transcriptions = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        audio_data = io.BytesIO(row['audio'])\n",
        "        transcription = return_transcription(audio_data)\n",
        "        transcriptions.append(transcription)\n",
        "\n",
        "    merged_df['transcription'] = transcriptions\n",
        "\n",
        "    # Transcription 결측치 제거\n",
        "    merged_df.dropna(subset=['transcription'], inplace=True)\n",
        "    merged_df = merged_df[merged_df['transcription'] != '']\n",
        "    merged_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Transcription 결측치 제거 후 화자 병합 재진행\n",
        "    final_merged_df = merge_speaker_rows(merged_df)\n",
        "    final_output_df = final_merged_df.drop(columns=[])\n",
        "\n",
        "    # 'speaker' -> '참석자 n' 으로 변경\n",
        "    final_output_df = rename_speakers(final_output_df)\n",
        "\n",
        "    print(f'Finished with transcribing, took {time.time() - start_time} sec\\n')\n",
        "\n",
        "    return final_output_df[['speakers', 'transcription', 'start_timestamp', 'end_timestamp']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "o-dMzLPwbCsK"
      },
      "outputs": [],
      "source": [
        "path = \"audio_path\" # 오디오 파일 경로\n",
        "num_speakers = 2 # 참석자 수가 2명일 경우\n",
        "\n",
        "df = speaker_diarize(path, num_speakers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ywVfF9lInR_W"
      },
      "outputs": [],
      "source": [
        "for idx, row in df.iterrows():\n",
        "    print(f\"{row['speakers']}: {row['transcription']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nFiWhnLvxPw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cSXRvKegizEd",
        "UjHCANFCi8kR",
        "iO2nX0LSj-nx",
        "Wxuzfg1YkD2I",
        "gNMRy5gHkgPU",
        "grA_AnktmvfD"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
